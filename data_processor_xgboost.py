"""
Data Processor XGBoost - Con m√©todo predecir_procesado integrado
"""

import pandas as pd
import numpy as np
import os
import joblib
from typing import Dict, Tuple

class DataProcessorXGBoost:
    """
    Procesador que carga y ejecuta el modelo XGBoost
    """
    
    def __init__(self, model_dir='.'):
        """Inicializa el procesador y carga el modelo"""
        self.model_dir = model_dir
        self.modelo = None
        self.scaler = None
        self.columnas_modelo = None
        
        # Cargar archivo de categor√≠as
        self.categorias = None
        self._cargar_categorias()
        
        # Cargar modelo
        self._cargar_modelo()
    
    def _cargar_categorias(self):
        """Carga el archivo de categor√≠as para mapear materias"""
        try:
            categorias_path = 'Ejemplo__1_.xlsx'
            
            if os.path.exists(categorias_path):
                self.categorias = pd.read_excel(categorias_path, sheet_name='Hoja1')
                # Crear diccionario de mapeo
                self.mapa_categorias = dict(zip(
                    self.categorias['Clase'], 
                    self.categorias['Categor√≠a ']
                ))
                print(f"‚úÖ Categor√≠as cargadas: {len(self.mapa_categorias)} materias mapeadas")
            else:
                print("‚ö†Ô∏è Archivo de categor√≠as no encontrado, usando mapeo por defecto")
                self.mapa_categorias = {}
                
        except Exception as e:
            print(f"‚ö†Ô∏è Error cargando categor√≠as: {e}")
            self.mapa_categorias = {}
    
    def _cargar_modelo(self):
        """Carga el modelo XGBoost y archivos auxiliares"""
        try:
            modelo_path = 'xgboost_modelo.pkl'
            scaler_path = 'scaler.pkl'
            columnas_path = 'columnas.pkl'
            
            print("üîç DEBUG: Iniciando carga del modelo...")
            print(f"   Ruta esperada: {modelo_path}")
            print(f"   Directorio actual: {os.getcwd()}")
            
            # Intentar descargar si no existe
            if not os.path.exists(modelo_path):
                print("üîç Modelo no encontrado localmente, intentando descargar...")
                self._descargar_modelo()
            
            # Verificar que existe despu√©s de descargar
            if not os.path.exists(modelo_path):
                raise FileNotFoundError(
                    f"‚ùå Modelo no encontrado: {modelo_path}\n"
                    f"   Verifica que el archivo se descarg√≥ correctamente de Google Drive"
                )
            
            print(f"‚úì Archivo encontrado: {modelo_path}")
            print(f"  Tama√±o: {os.path.getsize(modelo_path) / 1024 / 1024:.2f} MB")
            
            # Cargar modelo
            print("  Cargando modelo con joblib...")
            self.modelo = joblib.load(modelo_path)
            print("‚úÖ Modelo XGBoost cargado exitosamente")
            
            # Cargar scaler (opcional)
            if os.path.exists(scaler_path):
                self.scaler = joblib.load(scaler_path)
                print("‚úÖ Scaler cargado")
            else:
                self.scaler = None
                print("‚ö†Ô∏è  scaler.pkl no encontrado - continuando sin estandarizaci√≥n previa")
            
            # Cargar columnas (opcional)
            if os.path.exists(columnas_path):
                self.columnas_modelo = joblib.load(columnas_path)
                print("‚úÖ Columnas del modelo cargadas")
            else:
                self.columnas_modelo = None
                print("‚ö†Ô∏è  columnas.pkl no encontrado - usando todas las columnas disponibles")
            
        except Exception as e:
            print(f"‚ùå Error cargando modelo: {str(e)}")
            import traceback
            traceback.print_exc()
            self.modelo = None
            self.scaler = None
            self.columnas_modelo = None
    
    def _descargar_modelo(self):
        """Descarga el modelo desde Google Drive"""
        modelo_path = 'xgboost_modelo.pkl'
        
        if not os.path.exists(modelo_path):
            print("‚¨áÔ∏è Descargando modelo desde Google Drive...")
            print("   Tama√±o: ~142 MB - Esto puede tomar 1-2 minutos")
            
            try:
                import gdown
                
                file_id = "1VLySTpc2m4soxTEjTi7xUSJcXyrF00JF"
                
                try:
                    url = f"https://drive.google.com/uc?id={file_id}"
                    gdown.download(url, modelo_path, quiet=False, fuzzy=True)
                    print("‚úÖ Modelo descargado exitosamente")
                    return
                except Exception as e1:
                    print(f"‚ö†Ô∏è M√©todo 1 fall√≥: {str(e1)}")
                    
                    try:
                        gdown.cached_download(url, modelo_path, quiet=False)
                        print("‚úÖ Modelo descargado exitosamente (m√©todo 2)")
                        return
                    except Exception as e2:
                        print(f"‚ö†Ô∏è M√©todo 2 fall√≥: {str(e2)}")
                        print("   Intentando m√©todo 3 (requests)...")
                        import requests
                        
                        download_url = f"https://drive.google.com/uc?export=download&id={file_id}"
                        session = requests.Session()
                        response = session.get(download_url, stream=True)
                        
                        for key, value in response.cookies.items():
                            if key.startswith('download_warning'):
                                download_url = f"https://drive.google.com/uc?export=download&confirm={value}&id={file_id}"
                                response = session.get(download_url, stream=True)
                                break
                        
                        with open(modelo_path, 'wb') as f:
                            for chunk in response.iter_content(chunk_size=32768):
                                if chunk:
                                    f.write(chunk)
                        
                        print("‚úÖ Modelo descargado exitosamente (m√©todo 3)")
                        
            except Exception as e:
                print(f"‚ùå Error descargando: {str(e)}")
                print("   Soluci√≥n: Sube el archivo 'xgboost_modelo.pkl' manualmente")
        else:
            print("‚úÖ Modelo ya existe localmente")
    
    def predecir(self, data: pd.DataFrame) -> pd.DataFrame:
        """
        Realiza predicciones con el modelo XGBoost
        NOTA: Este m√©todo asume que los datos ya fueron procesados
        """
        print("\nüéØ INICIANDO PREDICCI√ìN...")
        print(f"   Estado del modelo: {'‚úÖ Cargado' if self.modelo is not None else '‚ùå NO cargado'}")
        print(f"   Tipo de modelo: {type(self.modelo).__name__}")
        print(f"   Registros a predecir: {len(data)}")
        
        if self.modelo is None:
            raise ValueError("‚ùå Modelo no cargado")
        
        print("üéØ Realizando predicciones...")
        
        try:
            # Preparar datos
            if self.columnas_modelo is not None:
                for col in self.columnas_modelo:
                    if col not in data.columns:
                        data[col] = 0
                X = data[self.columnas_modelo].copy()
            else:
                X = data.copy()
            
            print(f"   üìä Shape de X: {X.shape}")
            
            # Verificar duplicados
            if X.columns.duplicated().any():
                print("   ‚ö†Ô∏è Columnas duplicadas detectadas, eliminando...")
                X = X.loc[:, ~X.columns.duplicated()]
            
            # Asegurar que todo sea num√©rico
            for col in X.columns:
                if X[col].dtype == 'object':
                    X[col] = pd.to_numeric(X[col], errors='coerce').fillna(0)
            
            # Estandarizar
            if self.scaler is not None:
                print("   üîß Aplicando scaler...")
                X_scaled = self.scaler.transform(X)
            else:
                X_scaled = X.values
            
            print(f"   üìä Shape final: {X_scaled.shape}")
            
            # Predecir
            modelo_tipo = type(self.modelo).__name__
            
            if 'ExponentiatedGradient' in modelo_tipo:
                print("   ‚ÑπÔ∏è Detectado modelo con mitigaci√≥n (ExponentiatedGradient)")
                predicciones = self.modelo.predict(X_scaled)
                probabilidades = np.where(predicciones == 1, 0.9, 0.1)
            else:
                probabilidades = self.modelo.predict_proba(X_scaled)[:, 1]
            
            # Agregar resultados
            resultado = data.copy()
            resultado['probabilidad'] = probabilidades
            resultado['nivel_riesgo'] = pd.cut(
                probabilidades,
                bins=[0, 0.3, 0.6, 1.0],
                labels=["Bajo", "Medio", "Alto"]
            )
            
            print(f"‚úÖ Predicciones completadas: {len(resultado)} estudiantes")
            print(f"   üü¢ Bajo: {(resultado['nivel_riesgo']=='Bajo').sum()}")
            print(f"   üü° Medio: {(resultado['nivel_riesgo']=='Medio').sum()}")
            print(f"   üî¥ Alto: {(resultado['nivel_riesgo']=='Alto').sum()}")
            
            return resultado
            
        except Exception as e:
            print(f"‚ùå Error en predicci√≥n: {str(e)}")
            import traceback
            traceback.print_exc()
            raise
    
  def predecir_procesado(self, data_procesada: pd.DataFrame) -> pd.DataFrame:
        """
        Realiza predicciones con datos YA PROCESADOS por el pipeline integrado
        
        CORRECCI√ìN CR√çTICA: Calcula probabilidades correctamente para modelos
        ExponentiatedGradient con m√∫ltiples predictores y pesos.
        
        Args:
            data_procesada: DataFrame ya procesado (limpieza + encoding + ajustes)
            
        Returns:
            DataFrame con columnas adicionales:
            - probabilidad: Probabilidad de deserci√≥n (0-1)
            - nivel_riesgo: Nivel de riesgo ("Bajo", "Medio", "Alto")
        """
        print("\n" + "="*80)
        print("üéØ INICIANDO PREDICCI√ìN CON DATOS PROCESADOS")
        print("="*80)
        print(f"   üìä Registros recibidos: {len(data_procesada):,}")
        print(f"   üìä Columnas recibidas: {len(data_procesada.columns)}")
        
        # Verificar que el modelo est√© cargado
        if self.modelo is None:
            raise ValueError(
                "‚ùå Modelo XGBoost no cargado.\n"
                "   Verifica que 'xgboost_modelo.pkl' est√© en la ra√≠z del proyecto."
            )
        
        print(f"   ‚úÖ Modelo cargado: {type(self.modelo).__name__}")
        
        try:
            # ============================================================
            # PREPARACI√ìN DE DATOS
            # ============================================================
            
            X = data_procesada.copy()
            
            print("\nüîß Preparando datos para predicci√≥n...")
            
            # 1. Eliminar columnas problem√°ticas PRIMERO
            columnas_a_eliminar = []
            
            # Buscar desercion/deserci√≥n
            for col in X.columns:
                col_lower = col.lower()
                if 'desercion' in col_lower or 'deserci√≥n' in col_lower:
                    columnas_a_eliminar.append(col)
            
            # Buscar Estado (Dropout)
            for col in X.columns:
                if col == 'Estado (Dropout)' or col == 'Estado_Dropout':
                    columnas_a_eliminar.append(col)
            
            # Eliminar columnas identificadoras
            cols_id = ['ID', 'Mult Programa', 'Ciclo']
            for col in cols_id:
                if col in X.columns:
                    columnas_a_eliminar.append(col)
            
            if columnas_a_eliminar:
                X = X.drop(columns=columnas_a_eliminar, errors='ignore')
                print(f"   ‚úì {len(columnas_a_eliminar)} columnas eliminadas")
            
            # 2. Eliminar columnas no num√©ricas
            cols_object = X.select_dtypes(include=['object']).columns.tolist()
            if cols_object:
                X = X.drop(columns=cols_object)
            
            # 3. Eliminar columnas duplicadas
            if X.columns.duplicated().any():
                X = X.loc[:, ~X.columns.duplicated()]
            
            # 4. Manejar valores infinitos y NaN
            X = X.replace([np.inf, -np.inf], np.nan)
            if X.isnull().any().any():
                X = X.fillna(0)
            
            # 5. Alinear con columnas del modelo
            if self.columnas_modelo is not None:
                print(f"   ‚Üí Alineando con {len(self.columnas_modelo)} columnas del modelo")
                
                # Agregar columnas faltantes con 0
                for col in self.columnas_modelo:
                    if col not in X.columns:
                        X[col] = 0
                
                # Ordenar columnas seg√∫n el orden del entrenamiento (CR√çTICO)
                X = X[self.columnas_modelo]
            
            print(f"   ‚úÖ Datos preparados: {X.shape}")
            
            # ============================================================
            # APLICAR SCALER (CR√çTICO)
            # ============================================================
            
            if self.scaler is not None:
                print("\n   üîß Aplicando scaler (estandarizaci√≥n)...")
                X_scaled = self.scaler.transform(X)
                print(f"      ‚úì Datos escalados: {X_scaled.shape}")
            else:
                print("\n   ‚ö†Ô∏è NO HAY SCALER - Esto puede causar predicciones incorrectas")
                X_scaled = X.values
            
            # ============================================================
            # PREDICCI√ìN CON C√ÅLCULO CORRECTO DE PROBABILIDADES
            # ============================================================
            
            print("\nü§ñ Ejecutando predicci√≥n con XGBoost...")
            
            modelo_tipo = type(self.modelo).__name__
            print(f"   Tipo de modelo: {modelo_tipo}")
            
            # CORRECCI√ìN CR√çTICA: Calcular probabilidades correctamente
            if 'ExponentiatedGradient' in modelo_tipo:
                print("   ‚ÑπÔ∏è Modelo con mitigaci√≥n de sesgo detectado")
                
                # Predicciones de clase
                predicciones = self.modelo.predict(X_scaled)
                
                # CALCULAR PROBABILIDADES DESDE PREDICTORES INTERNOS
                if hasattr(self.modelo, 'predictors_') and hasattr(self.modelo, 'weights_'):
                    print(f"   üîç Calculando probabilidades desde {len(self.modelo.predictors_)} predictores")
                    
                    probabilidades_lista = []
                    
                    # Obtener probabilidades de cada predictor
                    for i, predictor in enumerate(self.modelo.predictors_):
                        proba = predictor.predict_proba(X_scaled)[:, 1]
                        probabilidades_lista.append(proba)
                    
                    # Promedio ponderado con los pesos del modelo
                    probabilidades = np.average(
                        probabilidades_lista,
                        axis=0,
                        weights=self.modelo.weights_
                    )
                    
                    print(f"   ‚úÖ Probabilidades calculadas con promedio ponderado")
                    print(f"      Pesos del modelo: {self.modelo.weights_[:5]}..." if len(self.modelo.weights_) > 5 else f"      Pesos: {self.modelo.weights_}")
                    
                else:
                    # Fallback: usar predicciones como probabilidades
                    print("   ‚ö†Ô∏è No se encontraron predictores internos, usando predicciones directas")
                    probabilidades = predicciones.astype(float)
            
            else:
                # Modelo est√°ndar (sin mitigaci√≥n)
                if hasattr(self.modelo, 'predict_proba'):
                    probabilidades = self.modelo.predict_proba(X_scaled)[:, 1]
                else:
                    predicciones = self.modelo.predict(X_scaled)
                    probabilidades = predicciones.astype(float)
            
            print(f"   ‚úÖ Predicciones generadas: {len(probabilidades):,}")
            
            # ============================================================
            # VALIDAR PROBABILIDADES
            # ============================================================
            
            print("\nüîç Validando probabilidades...")
            print(f"   Rango: [{probabilidades.min():.4f}, {probabilidades.max():.4f}]")
            print(f"   Media: {probabilidades.mean():.4f}")
            print(f"   Mediana: {np.median(probabilidades):.4f}")
            print(f"   Std: {probabilidades.std():.4f}")
            
            # Verificar si todas son iguales (problema detectado)
            valores_unicos = np.unique(probabilidades)
            if len(valores_unicos) == 1:
                print(f"   ‚ö†Ô∏è WARNING: Todas las probabilidades son {valores_unicos[0]:.4f}")
                print(f"   Esto indica un problema en el c√°lculo o datos")
            elif len(valores_unicos) < 10:
                print(f"   ‚ö†Ô∏è WARNING: Solo {len(valores_unicos)} valores √∫nicos de probabilidad")
                print(f"   Valores: {valores_unicos}")
            else:
                print(f"   ‚úÖ {len(valores_unicos):,} valores √∫nicos de probabilidad (correcto)")
            
            # ============================================================
            # AGREGAR RESULTADOS AL DATAFRAME
            # ============================================================
            
            resultado = data_procesada.copy()
            resultado['probabilidad'] = probabilidades
            
            # Clasificar nivel de riesgo
            resultado['nivel_riesgo'] = pd.cut(
                probabilidades,
                bins=[0, 0.3, 0.6, 1.0],
                labels=["Bajo", "Medio", "Alto"]
            )
            
            # ============================================================
            # ESTAD√çSTICAS FINALES
            # ============================================================
            
            print("\n" + "="*80)
            print("‚úÖ PREDICCI√ìN COMPLETADA")
            print("="*80)
            print(f"   üìä Estudiantes analizados: {len(resultado):,}")
            print(f"   üìä Probabilidad promedio: {probabilidades.mean():.2%}")
            print(f"   üìä Probabilidad m√≠nima: {probabilidades.min():.2%}")
            print(f"   üìä Probabilidad m√°xima: {probabilidades.max():.2%}")
            print(f"   üìä Desviaci√≥n est√°ndar: {probabilidades.std():.4f}")
            print("\n   üìà Distribuci√≥n de riesgo:")
            print(f"      üü¢ Bajo (<30%):   {(resultado['nivel_riesgo']=='Bajo').sum():>6,} estudiantes ({(resultado['nivel_riesgo']=='Bajo').sum()/len(resultado)*100:>5.1f}%)")
            print(f"      üü° Medio (30-60%): {(resultado['nivel_riesgo']=='Medio').sum():>6,} estudiantes ({(resultado['nivel_riesgo']=='Medio').sum()/len(resultado)*100:>5.1f}%)")
            print(f"      üî¥ Alto (>60%):    {(resultado['nivel_riesgo']=='Alto').sum():>6,} estudiantes ({(resultado['nivel_riesgo']=='Alto').sum()/len(resultado)*100:>5.1f}%)")
            print("="*80 + "\n")
            
            return resultado
            
        except Exception as e:
            print(f"\n‚ùå ERROR EN PREDICCI√ìN: {str(e)}")
            import traceback
            traceback.print_exc()
            raise

    
    def get_summary_stats(self, df: pd.DataFrame) -> dict:
        """Genera estad√≠sticas resumidas del dataframe procesado"""
        stats = {
            'total_estudiantes': len(df),
            'riesgo_bajo': (df['nivel_riesgo'] == 'Bajo').sum() if 'nivel_riesgo' in df.columns else 0,
            'riesgo_medio': (df['nivel_riesgo'] == 'Medio').sum() if 'nivel_riesgo' in df.columns else 0,
            'riesgo_alto': (df['nivel_riesgo'] == 'Alto').sum() if 'nivel_riesgo' in df.columns else 0,
        }
        return stats
